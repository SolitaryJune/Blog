---
layout:     post
title:      "字符集与编码"
subtitle:   "encode,deconde,character-set"
date:       2017-12-01
author:     "elmagnifico"
header-img: "img/python-head-bg.png"
catalog:    true
tags:
    - python
---

# Foreword

之前遇到了各种类似于下面的问题

```
UnicodeEncodeError: 'gbk' codec can't encode character
UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1....
```

每次都是临时搜一下找一下解决办法,然后弄过来,好用了就不管了,下次再遇到再搜一下...

所以一直没搞清楚到底是为啥出错了,现在来梳理一下会用到的各种编码格式以及对应的字符集

## 字符集与编码

首先要分清楚什么是字符集什么是编码格式

#### 字符集

字符集,很容易理解他就是一本字典,每一个符合(英文,中文,数学符号等等),他们都与某一个数字对应,从而构成了字符集.

比如在ASCII中'0'对应数字48,'a'对应数字97,这样一组构成了一个字符集

#### 编码格式

编码格式,其本质上是如何存储对应的字符集,比如ASCII中'a'对应的是数字97,那么其存储在内存/硬盘中也是用二进制表示出来的97.当然有很多种编码格式,不同的编码格式对应的并不一定都是按照其数字来存储的,所以编码规定的是字符集如何存储在介质中.

分清了这两个东西才能明白下面的内容。

#### ASCII与其扩展

首先ASCII编码（非扩展）其即是字符集也是编码格式，他同时定义了两个。

他在存储的时候只用了1byte，并且其也只用了低7位，开始时只定义了128个字符。

标准的ASCII只是英语系国际可以使用，对于其他国际而言这样的字符根本无法表达本族语言，所以对此出现了其扩展。

而ASCII标准下刚好又只用了128个字符，所以其他国家就接过来自定义了自己国家的字符，从而有了不同国家的不同扩展，但是这样依然不够用，比如汉字就根本无法用128个字符表达出来，针对这种情况呢，自然就有了新的字符集GBK

#### GBK

GBK,它类似ASCII即是字符集也是编码格式,具体的我就不说了,只需要知道他是用两个字节,也就是16bits,也就是最多能表示65535个字符(实际上没用这么多).而且呢,GBK也兼容ASCII,从而可以在一个文章中既有汉字有有英文并且不会发生错乱的情况.

但是GBK只是用来兼容了中文和英文,其他国家的语言也同样需要这样的规范,这样就出现了,一个国家一个字符集(兼容ASCII),可能在存储的时候同样的一个数字,但是实际上表示含义在不同国家并不相同,这样就很容易引起误会,因为别人并不知道你这个文件到底用哪国的字符集和编码形式.

#### Unicode

基于上面的需求,从而出现了大一统,那就是Unicode,Unicode就是一个单纯的字符集,他不仅仅收录了英文,中文,还收录了绝大多数国家的语言符号于其中,其使用了2的32位(4字节)的大小来存储字符,从而保证了基本不可能用完的字符表示.

这里只规定了字符集,可以看到如果真的要存储Unicode编码的字符,需要四字节,但是不同的国家的字符,比如英文完全不需要四个字节来存储,这样会浪费非常多的空间,所以这里还差一个编码格式,下面来介绍

#### UTF-8/16/32

这里的UTF就是对应的Unicode的编码方式,其中UTF-8所使用的编码规则是按照最小单位为字节进行编码,比如英文只需要一个单位,但是汉字等就需要3个字节(所以有时候文件会变大).

UTF-16则是使用最小单位是2个字节来编码,存储的时候自然会遇到大端小端的问题.

UTF-32则是使用最小单位是4个字节来编码,那么存储的时候依然也有大小端问题,但是对于code来说却最方便了,因为大小都是一致的，每4个字节解析一下就好了。

所以一般情况下UTF-8使用是最为广泛的，其兼顾了很多东西





ANSI 微软自己的称呼,实际上会根据系统语言选项改变对应的字符集和编码形式

而记事本的ANSI编码，就是这种默认编码，所以，一个中文文本，用ANSI编码保存，在中文版里编码是GBK模式保存的时候，到繁体中文版里，用BIG5读取，就全乱套了。记事本也不甘心这样，所以它要支持Unicode，但是有一个问题，一段二进制编码，如何确定它是GBK还是BIG5还是UTF-16/UTF-8？记事本的做法是在TXT文件的最前面保存一个标签，如果记事本打开一个TXT，发现这个标签，就说明是unicode。标签叫BOM，如果是0xFF 0xFE，是UTF16LE，如果是0xFE 0xFF则UTF16BE，如果是0xEF 0xBB 0xBF，则是UTF-8。如果没有这三个东西，那么就是ANSI，使用操作系统的默认语言编码来解释。

作者：时国怀
链接：https://www.zhihu.com/question/20650946/answer/15751688
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


ASCII 一般编码 英语系  可用于存储 字符集与编码
扩展ASCII 不统一 给非英语系使用 可用于存储 字符集与编码
GB2312 中文对应的编码,不用于存储 字符集
unicode 统一编码,有每个字对应的二进制,但是过于臃肿 不用于存储,只是一个广泛的定义 字符集

下面都是编码格式
UTF-8 基于unicode 修改了臃肿的编码形式,用于存储
UTF-16
UTF-32

unicode --一定的规则---> UTF-8



网页编程中用不用bom我就不说什么了，因为软件原因无法使用的就更不能用了。最近在学习用cocos2d-x，纯C++的编码，如果代码中有中文等的非ascii字符出现。发现会出错。代码是在mac 下用xcode 写的，放到windows 下用vs 编译。最后把所有的源文件转成了带bom的格式后编译通过了，链接失败，这想这个就不是编码的问题了。通常情况下，一般都 会认为在写C++代码的时候不要用中文，但是很多时候我们程序员也有想自己看着舒服的时候，为神马就不能写中文了？于是在windows 下写了一个helloworld.cpp 类型的文件，输出内容用中文，然后存为utf-8 带bom格式，再把它copy到mac 下用g++ 编译，发现成功通过并且可正常运行，用xcode打开源文件也正常显示。所以，这里建议程序要在windows 和 mac 还有linux 上运行的话，源代码最好保存成utf-8 带bom的格式，这样比较通用一些。而用utf-16 无论大端还是小端，g++ 都不认的。或者用utf-8 不带bom格式，然后代码不要出现非ascii 127以后的字符。关于说utf-8 不带bom 才是标准的，我想应该是带用个人情绪的说法吧。真正的标准应该是bom是可选的，为什么可选?因为有些时候不带bom会出错，就拿历史较久远的windows来讲吧，很多国家的用户都在用windows ，其文件都是用其本地的ansi 编码来做的，比如大陆的GBK和GB2013,港台的big5，这些编码因为针对当地所用的字符制定的，所以呢，其存储文件较小，所以会大量使用，并且也大量存在着，微软不可能不考虑全球几十亿的用户的文件而盲目地修改解码方式，并且微软也是uncode 制定者之一，所以，带用bom的utf-8也是符合国际标准的。或许是因为程序编写者的个人原因，也许是考虑到效率，很多的程序无法正确区分一个utf-8文件是否有bom，所以导致了各种乱码的出现。个人不想说哪个是标准，也不想用语言去攻击哪个公司或团体。微软在坚持使用bom上没有错，因为这是在为用户考虑的。也许给我们这些写程序的带来了不便，但是，计算机最广泛的用户不是程序员。


## pyinstaller


##



## others

# Summary


# Quote

> https://www.cnblogs.com/fnng/p/5008884.html
>
> http://www.jb51.net/article/64816.htm
>
> http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html
> http://www.imkevinyang.com/2009/02/%E5%AD%97%E7%AC%A6%E7%BC%96%E8%A7%A3%E7%A0%81%E7%9A%84%E6%95%85%E4%BA%8B%EF%BC%88ascii%EF%BC%8Cansi%EF%BC%8Cunicode%EF%BC%8Cutf-8%E5%8C%BA%E5%88%AB%EF%BC%89.html
>
> https://www.cnblogs.com/tingyugetc/p/5727383.html
>
> https://www.zhihu.com/question/20312182
>
> https://www.zhihu.com/question/20167122/answer/23328115
